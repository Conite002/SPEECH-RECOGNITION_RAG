import os, json, subprocess
import ollama
from ollama import ChatResponse
from ollama import chat



def sanitize_filename(filename):
    """Sanitizes filenames for saving."""
    return "".join(c if c.isalnum() else "_" for c in filename)

def log(message):
    """Logs messages to the console."""
    print(f"[LOG]: {message}")

def build_custom_prompt_with_sources(query, context, sources):
    """
    Dynamically builds a prompt based on the query, context, and sources.
    Args:
        query (str): The user query.
        context (str): The retrieved context.
        sources (list): List of sources.
    Returns:
        str: The dynamically constructed prompt.
    """
    if not context and not sources:  # Simple query, no context or sources
        return f"You are a conversational assistant. Respond to the following query: {query}"

    # If context and sources are available
    sources_text = "\n".join([f"- {source}" for source in sources])
    prompt = f"""
    You are an expert assistant in Automatic Speech Recognition (ASR) with deep knowledge of the latest 
    machine learning techniques and their applications.

    Below, I will provide you with some **context** extracted from relevant documents, along with their **sources**. 
    Use this context and these sources to answer the **question** provided.

    ### Context:
    {context if context else "No context provided."}

    ### Sources:
    {sources_text if sources else "No sources provided."}

    ### Question:
    {query}

    ### Instructions:
    - If the context is insufficient to answer the question, politely say, "I need more information to answer this question."
    - Format the response as follows:
      - Short summary (1-2 sentences).
      - Detailed explanation in paragraphs or bullet points.
      - Reference sources wherever applicable (e.g., "According to [source]"). 
    """
    return prompt

def generate_response_with_retry(prompt, retries=3):
    """
    Generates a response using the LLM with retries in case of failures.
    Args:
        prompt (str): The structured prompt for the LLM.
        retries (int): Number of retry attempts in case of failure.
    Returns:
        str: Generated response or error message.
    """
    for attempt in range(retries):
        try:
            response = generate_response_with_prompt(prompt)
            if response != "Error: Invalid JSON response.":
                return response
            log(f"Attempt {attempt + 1}/{retries} failed: Invalid JSON response.")
        except Exception as e:
            log(f"Attempt {attempt + 1}/{retries} failed: {e}")
        if attempt < retries - 1:
            log(f"Retrying to generate response ({attempt + 1}/{retries})...")
    return "Failed to generate response after multiple attempts."



def generate_response_with_prompt(prompt):
    """
    Generates a response using the Llama model via the Ollama `chat` function.
    Args:
        prompt (str): The structured prompt to send to the LLM.
    Returns:
        str: The full response generated by the model.
    """
    response = None 
    try:
        log(f"Sending prompt to model: {prompt}")
        response = chat(model="llama3.2", messages=[{"role": "user", "content": prompt}])

        # Extract content from the response
        full_response = ""
        return response['message']['content']
    
    except Exception as e:
        print(f"Error during LLM interaction: {e}")
        return "Error: Failed to generate response."
